{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process of breaking-up the given text into small units such as sentences or words.\n",
    "*  Tokenization does this task by locating word boundaries\n",
    "*  Ending point of a word and beginning of the next word is called word boundary.\n",
    "*  Also knows as word segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges in Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Depends on the type of language\n",
    "   * English, French are reffered as space-delimited\n",
    "       * Most of the words are separated from each other by whited spaces\n",
    "   * Chinese, Thai are reffered to as unsegmented because words do not have slear boundaries\n",
    "   \n",
    "2. Tokenization is also affected by writing system and the typographical structure of the words.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rck/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize #will be use to tokenize the sentences\n",
    "from nltk.tokenize import word_tokenize #will be use to tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "\n",
    "dateset = \"\"\"They’ve perched for hours\n",
    "on that window-ledge, scarcely   \n",
    "moving. Beak to beak,\n",
    "\n",
    "a matched set, they differ   \n",
    "almost imperceptibly—\n",
    "like salt and pepper shakers.\n",
    "\n",
    "It’s an event when they tuck   \n",
    "(simultaneously) their pinpoint   \n",
    "heads into lavender vests\n",
    "\n",
    "of fat. But reminiscent   \n",
    "of clock hands blandly   \n",
    "turning because they must\n",
    "\n",
    "have turned—somehow, they’ve   \n",
    "taken...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They’ve perched for hours\\non that window-ledge, scarcely   \\nmoving.', 'Beak to beak,\\n\\na matched set, they differ   \\nalmost imperceptibly—\\nlike salt and pepper shakers.', 'It’s an event when they tuck   \\n(simultaneously) their pinpoint   \\nheads into lavender vests\\n\\nof fat.', 'But reminiscent   \\nof clock hands blandly   \\nturning because they must\\n\\nhave turned—somehow, they’ve   \\ntaken...']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the Sentences\n",
    "\n",
    "print(sent_tokenize(dateset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They’ve perched for hours\n",
      "on that window-ledge, scarcely   \n",
      "moving.\n",
      "Beak to beak,\n",
      "\n",
      "a matched set, they differ   \n",
      "almost imperceptibly—\n",
      "like salt and pepper shakers.\n",
      "It’s an event when they tuck   \n",
      "(simultaneously) their pinpoint   \n",
      "heads into lavender vests\n",
      "\n",
      "of fat.\n",
      "But reminiscent   \n",
      "of clock hands blandly   \n",
      "turning because they must\n",
      "\n",
      "have turned—somehow, they’ve   \n",
      "taken...\n"
     ]
    }
   ],
   "source": [
    "for i in sent_tokenize(dateset):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', '’', 've', 'perched', 'for', 'hours', 'on', 'that', 'window-ledge', ',', 'scarcely', 'moving', '.', 'Beak', 'to', 'beak', ',', 'a', 'matched', 'set', ',', 'they', 'differ', 'almost', 'imperceptibly—', 'like', 'salt', 'and', 'pepper', 'shakers', '.', 'It', '’', 's', 'an', 'event', 'when', 'they', 'tuck', '(', 'simultaneously', ')', 'their', 'pinpoint', 'heads', 'into', 'lavender', 'vests', 'of', 'fat', '.', 'But', 'reminiscent', 'of', 'clock', 'hands', 'blandly', 'turning', 'because', 'they', 'must', 'have', 'turned—somehow', ',', 'they', '’', 've', 'taken', '...']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the Words\n",
    "print(word_tokenize(dateset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They\n",
      "’\n",
      "ve\n",
      "perched\n",
      "for\n",
      "hours\n",
      "on\n",
      "that\n",
      "window-ledge\n",
      ",\n",
      "scarcely\n",
      "moving\n",
      ".\n",
      "Beak\n",
      "to\n",
      "beak\n",
      ",\n",
      "a\n",
      "matched\n",
      "set\n",
      ",\n",
      "they\n",
      "differ\n",
      "almost\n",
      "imperceptibly—\n",
      "like\n",
      "salt\n",
      "and\n",
      "pepper\n",
      "shakers\n",
      ".\n",
      "It\n",
      "’\n",
      "s\n",
      "an\n",
      "event\n",
      "when\n",
      "they\n",
      "tuck\n",
      "(\n",
      "simultaneously\n",
      ")\n",
      "their\n",
      "pinpoint\n",
      "heads\n",
      "into\n",
      "lavender\n",
      "vests\n",
      "of\n",
      "fat\n",
      ".\n",
      "But\n",
      "reminiscent\n",
      "of\n",
      "clock\n",
      "hands\n",
      "blandly\n",
      "turning\n",
      "because\n",
      "they\n",
      "must\n",
      "have\n",
      "turned—somehow\n",
      ",\n",
      "they\n",
      "’\n",
      "ve\n",
      "taken\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(dateset):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
